# Migration Plan: v1.0 â†’ v2.0

**Document Version:** 1.0
**Created:** 2025-11-21
**Status:** Planning Phase
**Target Completion:** 20 weeks (5 months)

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Current State Assessment](#current-state-assessment)
3. [Target State Architecture](#target-state-architecture)
4. [Migration Strategy](#migration-strategy)
5. [Phase-by-Phase Migration](#phase-by-phase-migration)
6. [Data Migration](#data-migration)
7. [Rollback Procedures](#rollback-procedures)
8. [Risk Mitigation](#risk-mitigation)
9. [Testing Strategy](#testing-strategy)
10. [Deployment Strategy](#deployment-strategy)
11. [Timeline and Milestones](#timeline-and-milestones)
12. [Success Criteria](#success-criteria)

---

## Executive Summary

### Migration Goals

Transform the Learning Voice Agent from a simple voice conversation system (v1.0) into a production-grade, multi-agent AI learning platform (v2.0) with:

- **Multi-agent orchestration** using LangGraph/CrewAI
- **Semantic memory** with vector embeddings and RAG
- **Multi-modal support** (voice + vision + documents)
- **Real-time learning** and model improvement
- **Cross-device synchronization** (web + mobile)
- **Production-ready infrastructure** on Railway

### Approach

**Incremental migration** with parallel operation:
- Maintain v1.0 in production during rebuild
- Build v2.0 components in parallel
- Gradual traffic migration with feature flags
- Zero-downtime deployment
- Data migration with backward compatibility

### Key Risks

1. **Data loss** during migration
2. **Service disruption** during deployment
3. **API breaking changes** affecting clients
4. **Cost escalation** with new infrastructure
5. **Technical debt** from rushed migration

### Mitigation Strategy

- Comprehensive backup procedures
- Incremental rollout with rollback plans
- API versioning (/v1, /v2 endpoints)
- Cost monitoring and alerts
- Test-driven development throughout

---

## Current State Assessment

### System Health (v1.0)

**Health Score:** 75/100 ğŸŸ¡

**Strengths:**
- âœ… Core functionality working
- âœ… Clean SPARC architecture
- âœ… All imports resolved
- âœ… Basic documentation complete
- âœ… Async/await patterns throughout

**Weaknesses:**
- âš ï¸ No authentication/authorization
- âš ï¸ Print statements instead of logging
- âš ï¸ Low test coverage (~10%)
- âš ï¸ SQLite scalability limits
- âš ï¸ No horizontal scaling support
- âš ï¸ Limited error handling

### Technical Debt

**Total Estimated Debt:** 30.5 hours

**Critical Issues (1):**
1. Logging system incomplete (8 hours)

**High Priority (4):**
1. No circuit breakers for APIs (4 hours)
2. Missing integration tests (6 hours)
3. No database migrations (3 hours)
4. Security vulnerabilities (5 hours)

**Medium Priority (8):**
- Rate limiting needed
- API documentation incomplete
- No monitoring/alerting
- Backup strategy missing
- Error handling incomplete

### Dependency Analysis

**Current Dependencies (17 packages):**
```
Core:
- fastapi==0.109.0
- uvicorn==0.27.0
- anthropic==0.18.1
- openai==1.10.0
- redis==5.0.1
- aiosqlite==0.19.0
- pydantic==2.5.3

Optional:
- twilio==8.11.0
- tenacity>=8.2.0
- circuitbreaker>=1.4.0
- slowapi>=0.1.9
```

**v2.0 New Dependencies:**
```
AI/ML:
- langchain>=0.1.0
- langgraph>=0.0.30
- chromadb>=0.4.0
- sentence-transformers>=2.2.0
- neo4j>=5.14.0

Database:
- asyncpg>=0.29.0 (PostgreSQL)
- alembic>=1.13.0 (migrations)

Frontend:
- react>=18.2.0
- next>=14.0.0
- react-native>=0.73.0

Infrastructure:
- sentry-sdk>=1.40.0
- prometheus-client>=0.19.0
```

### Data Inventory

**SQLite Database:**
- **Captures table:** ~0 rows (new deployment)
- **FTS5 index:** Auto-synced with captures
- **Database size:** < 1MB
- **Backup:** None currently

**Redis Data:**
- **Session contexts:** Ephemeral (30-min TTL)
- **Session metadata:** Ephemeral
- **Total keys:** ~0 (new deployment)

---

## Target State Architecture

### High-Level Vision

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRESENTATION LAYER                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Next.js Web â”‚ React Native â”‚  Twilio      â”‚   Public API     â”‚
â”‚   (Vercel)   â”‚  (iOS/And.)  â”‚  (Voice)     â”‚   (FastAPI)      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚               â”‚              â”‚               â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚           ORCHESTRATION LAYER (LangGraph)               â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚  ConversationAgent | AnalysisAgent | ResearchAgent      â”‚
       â”‚  VisionAgent | SynthesisAgent                           â”‚
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚              INTELLIGENCE LAYER                          â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚ Claude 3.5 â”‚  Whisper     â”‚  GPT-4V     â”‚  Fine-tuned   â”‚
       â”‚  (Sonnet)  â”‚  (Audio)     â”‚  (Vision)   â”‚   Models      â”‚
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚            â”‚             â”‚                â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                 MEMORY SYSTEMS                           â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚  ChromaDB  â”‚   Neo4j      â”‚  PostgreSQL  â”‚     Redis     â”‚
       â”‚  (Vectors) â”‚   (Graph)    â”‚  (Primary)   â”‚    (Cache)    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚            INFRASTRUCTURE (Railway)                      â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚  Auto-scaling | Monitoring | Backup | CDN               â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Improvements Over v1.0

| Capability | v1.0 | v2.0 |
|------------|------|------|
| **AI Model** | Claude Haiku | Claude 3.5 Sonnet |
| **Agents** | Single | Multi-agent (5+) |
| **Memory** | Redis only | Vector + Graph + SQL |
| **Search** | Keyword (FTS5) | Semantic (embeddings) |
| **Input** | Voice + Text | Voice + Vision + Docs |
| **Frontend** | Vue 3 PWA | Next.js + React Native |
| **Database** | SQLite | PostgreSQL + ChromaDB |
| **Scaling** | Single instance | Auto-scaling |
| **Auth** | None | JWT + API keys |
| **Analytics** | None | Full insights engine |

---

## Migration Strategy

### Approach: Incremental with Parallel Systems

**Strategy:** Run v1.0 and v2.0 in parallel, gradually migrate traffic.

**Rationale:**
- Minimize risk of data loss
- Zero-downtime deployment
- Easy rollback if issues arise
- Gradual user migration
- Time to validate v2.0 stability

### Migration Phases

```
Phase 1: Foundation (Weeks 1-2)
â”œâ”€ Stabilize v1.0
â”œâ”€ Add comprehensive tests
â”œâ”€ Implement monitoring
â””â”€ Document architecture

Phase 2: Multi-Agent Core (Weeks 3-4)
â”œâ”€ Build LangGraph orchestration
â”œâ”€ Implement conversation agents
â”œâ”€ Set up Flow Nexus integration
â””â”€ Deploy alongside v1.0

Phase 3: Vector Memory (Weeks 5-6)
â”œâ”€ Set up ChromaDB
â”œâ”€ Implement embedding pipeline
â”œâ”€ Build knowledge graph (Neo4j)
â””â”€ Migrate historical data

Phase 4: Multi-Modal (Weeks 7-8)
â”œâ”€ Integrate GPT-4V
â”œâ”€ Build document processing
â””â”€ Unified input handler

Phase 5: Real-Time Learning (Weeks 9-10)
â”œâ”€ Feedback collection
â”œâ”€ Training pipeline
â””â”€ Model versioning

Phase 6: Analytics Engine (Weeks 11-12)
â”œâ”€ Pattern detection
â”œâ”€ Insights dashboard
â””â”€ Recommendations

Phase 7: Modern Frontend (Weeks 13-14)
â”œâ”€ Next.js rebuild
â”œâ”€ Real-time updates
â””â”€ Mobile-responsive

Phase 8: Mobile Apps (Weeks 15-16)
â”œâ”€ React Native setup
â”œâ”€ Voice/camera integration
â””â”€ Offline-first architecture

Phase 9: Cross-Device Sync (Weeks 17-18)
â”œâ”€ CRDT sync protocol
â”œâ”€ Conflict resolution
â””â”€ Encryption

Phase 10: Production Deploy (Weeks 19-20)
â”œâ”€ Railway configuration
â”œâ”€ Load testing
â”œâ”€ Cut over to v2.0
â””â”€ Decommission v1.0
```

### Parallel Operation Strategy

**Endpoint Versioning:**
```
v1.0 endpoints:
- /api/conversation â†’ v1 backend
- /api/search â†’ v1 SQLite FTS5
- /ws/{session} â†’ v1 WebSocket

v2.0 endpoints:
- /v2/conversation â†’ v2 multi-agent
- /v2/search â†’ v2 vector search
- /v2/ws/{session} â†’ v2 WebSocket

Shared:
- /health â†’ Both versions
- /metrics â†’ Aggregated
```

**Feature Flags:**
```python
# app/config.py
class Settings:
    enable_v2_agents: bool = False
    enable_vector_search: bool = False
    enable_vision: bool = False
    v2_traffic_percentage: int = 0  # 0-100
```

**Gradual Traffic Migration:**
```
Week 1-2:   v2 = 0%   (v1 only)
Week 3-4:   v2 = 5%   (beta testers)
Week 5-8:   v2 = 25%  (early adopters)
Week 9-12:  v2 = 50%  (half traffic)
Week 13-16: v2 = 75%  (majority)
Week 17-18: v2 = 95%  (almost all)
Week 19-20: v2 = 100% (full cutover)
```

---

## Phase-by-Phase Migration

### Phase 1: Foundation (Weeks 1-2)

**Goal:** Stabilize v1.0 and prepare infrastructure.

**Tasks:**

1. **Fix Critical Bugs**
   - [ ] Replace print() with structured logging
   - [ ] Add circuit breakers for AI APIs
   - [ ] Implement comprehensive error handling
   - [ ] Add input validation and sanitization

2. **Test Suite**
   - [ ] Unit tests for all modules (80% coverage)
   - [ ] Integration tests for API endpoints
   - [ ] Load tests for performance baselines
   - [ ] CI/CD pipeline with GitHub Actions

3. **Monitoring**
   - [ ] Integrate Sentry for error tracking
   - [ ] Add Prometheus metrics
   - [ ] Set up health check endpoints
   - [ ] Create dashboards (Railway or Grafana)

4. **Documentation**
   - [x] ARCHITECTURE_V1.md (current system)
   - [x] MIGRATION_PLAN.md (this document)
   - [x] API_DOCUMENTATION.md
   - [x] DEVELOPMENT_GUIDE.md
   - [x] DEPLOYMENT_GUIDE.md

**Deliverables:**
- Stable v1.0 in production
- Comprehensive test suite
- Monitoring dashboards
- Complete documentation

**Success Criteria:**
- Zero critical bugs
- 80%+ test coverage
- < 0.1% error rate
- All docs complete

### Phase 2: Multi-Agent Core (Weeks 3-4)

**Goal:** Build LangGraph orchestration with multiple agents.

**Tasks:**

1. **Agent Architecture**
   - [ ] Design agent coordination protocol
   - [ ] Implement ConversationAgent (Claude 3.5 Sonnet)
   - [ ] Create AnalysisAgent (concept extraction)
   - [ ] Build ResearchAgent (tool integration)
   - [ ] Set up LangGraph orchestrator

2. **Flow Nexus Integration**
   - [ ] Configure swarm topology
   - [ ] Implement agent communication
   - [ ] Add neural pattern training
   - [ ] Set up performance monitoring

3. **API Development**
   - [ ] Create /v2/conversation endpoint
   - [ ] Implement feature flags
   - [ ] Add versioning middleware
   - [ ] Update API documentation

**Data Impact:**
- No migration needed (new endpoints)
- v1 data remains in SQLite
- New agent metadata in PostgreSQL

**Deliverables:**
- Working multi-agent system
- v2 API endpoints live (0% traffic)
- Agent coordination dashboard

**Rollback Plan:**
- Disable v2 feature flags
- Route all traffic to v1
- Keep v1 unchanged

### Phase 3: Vector Memory (Weeks 5-6)

**Goal:** Add semantic memory with vector search.

**Tasks:**

1. **Infrastructure**
   - [ ] Deploy ChromaDB instance
   - [ ] Set up Neo4j graph database
   - [ ] Configure PostgreSQL primary DB
   - [ ] Implement Alembic migrations

2. **Embedding Pipeline**
   - [ ] Choose embedding model (sentence-transformers)
   - [ ] Build batch embedding generator
   - [ ] Create ChromaDB collection schema
   - [ ] Implement hybrid search (vector + keyword)

3. **Knowledge Graph**
   - [ ] Design Neo4j schema (concepts, relationships)
   - [ ] Build concept extraction pipeline
   - [ ] Create graph traversal queries
   - [ ] Implement graph-based context retrieval

4. **RAG Implementation**
   - [ ] Context retrieval with embeddings
   - [ ] Prompt augmentation with relevant docs
   - [ ] Response generation with context
   - [ ] Relevance scoring and filtering

**Data Migration:**

```python
# Migration script: migrate_to_vector_db.py

async def migrate_sqlite_to_vector():
    """Migrate SQLite captures to ChromaDB + PostgreSQL"""

    # 1. Connect to both databases
    sqlite_conn = await aiosqlite.connect('learning_captures.db')
    pg_pool = await asyncpg.create_pool(DATABASE_URL)
    chroma_client = chromadb.Client()

    # 2. Create collections
    collection = chroma_client.create_collection(
        name="captures",
        metadata={"description": "Conversation captures"}
    )

    # 3. Read all SQLite captures
    async with sqlite_conn.execute('SELECT * FROM captures') as cursor:
        async for row in cursor:
            # 4. Generate embedding
            text = f"{row['user_text']} {row['agent_text']}"
            embedding = generate_embedding(text)

            # 5. Store in ChromaDB
            collection.add(
                embeddings=[embedding],
                documents=[text],
                metadatas=[{
                    'session_id': row['session_id'],
                    'timestamp': row['timestamp']
                }],
                ids=[str(row['id'])]
            )

            # 6. Store in PostgreSQL
            await pg_pool.execute('''
                INSERT INTO captures
                (id, session_id, timestamp, user_text, agent_text, metadata)
                VALUES ($1, $2, $3, $4, $5, $6)
            ''', row['id'], row['session_id'], ...)

    print(f"Migrated {count} captures to vector DB")
```

**Verification:**
- Compare record counts (SQLite vs PostgreSQL)
- Test semantic search accuracy
- Verify embedding quality
- Check query performance

**Deliverables:**
- ChromaDB with all historical captures
- PostgreSQL as primary database
- Neo4j knowledge graph
- Semantic search API (/v2/search)

**Rollback Plan:**
- Keep SQLite database intact
- Switch VECTOR_SEARCH feature flag to False
- Fall back to FTS5 search

### Phase 4-10: Remaining Phases

*(See REBUILD_STRATEGY.md for detailed phase breakdowns)*

**Key Points:**
- Each phase builds on previous
- Maintain v1 operation throughout
- Incremental feature rollout
- Continuous testing and validation

---

## Data Migration

### Migration Scope

**Data to Migrate:**

1. **Conversation Captures** (SQLite â†’ PostgreSQL + ChromaDB)
   - User text and agent responses
   - Session metadata
   - Timestamps
   - Source information

2. **Session State** (Redis â†’ PostgreSQL sessions table)
   - Active session contexts
   - User preferences
   - Conversation history

3. **User Data** (New in v2.0)
   - User accounts (new)
   - API keys (new)
   - Device registrations (new)

### Migration Procedure

**Step 1: Backup**

```bash
# Backup SQLite database
cp learning_captures.db learning_captures_backup_$(date +%Y%m%d).db

# Backup Redis data
redis-cli --rdb redis_backup_$(date +%Y%m%d).rdb

# Upload backups to R2
aws s3 cp learning_captures_backup_*.db s3://backups/
```

**Step 2: Schema Creation**

```sql
-- PostgreSQL schema for v2.0

-- Users table (new)
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255),
    created_at TIMESTAMP DEFAULT NOW(),
    last_login TIMESTAMP
);

-- Captures table (migrated from SQLite)
CREATE TABLE captures (
    id BIGSERIAL PRIMARY KEY,
    user_id UUID REFERENCES users(id),
    session_id VARCHAR(255) NOT NULL,
    timestamp TIMESTAMP DEFAULT NOW(),
    user_text TEXT NOT NULL,
    agent_text TEXT NOT NULL,
    metadata JSONB,
    embedding_id VARCHAR(255),  -- ChromaDB reference

    INDEX idx_user_session (user_id, session_id),
    INDEX idx_timestamp (timestamp DESC)
);

-- Sessions table
CREATE TABLE sessions (
    id VARCHAR(255) PRIMARY KEY,
    user_id UUID REFERENCES users(id),
    created_at TIMESTAMP DEFAULT NOW(),
    last_activity TIMESTAMP DEFAULT NOW(),
    metadata JSONB
);

-- Concepts table (for knowledge graph)
CREATE TABLE concepts (
    id BIGSERIAL PRIMARY KEY,
    name VARCHAR(255) UNIQUE NOT NULL,
    description TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Capture-Concept relationships
CREATE TABLE capture_concepts (
    capture_id BIGINT REFERENCES captures(id),
    concept_id BIGINT REFERENCES concepts(id),
    weight FLOAT DEFAULT 1.0,

    PRIMARY KEY (capture_id, concept_id)
);
```

**Step 3: Data Transfer**

```python
# Full migration script

import asyncio
import aiosqlite
import asyncpg
from chromadb import Client
from sentence_transformers import SentenceTransformer

async def full_migration():
    # Initialize connections
    sqlite = await aiosqlite.connect('learning_captures.db')
    postgres = await asyncpg.create_pool(PG_URL)
    chroma = Client()
    embedder = SentenceTransformer('all-MiniLM-L6-v2')

    # Create ChromaDB collection
    collection = chroma.get_or_create_collection('captures')

    # Migrate captures
    async with sqlite.execute('SELECT * FROM captures ORDER BY id') as cursor:
        batch = []
        async for row in cursor:
            # Prepare data
            text = f"{row['user_text']} {row['agent_text']}"
            embedding = embedder.encode(text)

            # Add to batch
            batch.append({
                'id': row['id'],
                'session_id': row['session_id'],
                'timestamp': row['timestamp'],
                'user_text': row['user_text'],
                'agent_text': row['agent_text'],
                'metadata': row['metadata'],
                'text': text,
                'embedding': embedding
            })

            # Process in batches of 100
            if len(batch) >= 100:
                await migrate_batch(postgres, collection, batch)
                batch = []

        # Process remaining
        if batch:
            await migrate_batch(postgres, collection, batch)

    print("Migration complete!")

async def migrate_batch(pg, chroma, batch):
    # Insert into PostgreSQL
    await pg.executemany('''
        INSERT INTO captures
        (id, session_id, timestamp, user_text, agent_text, metadata)
        VALUES ($1, $2, $3, $4, $5, $6)
    ''', [(b['id'], b['session_id'], ...) for b in batch])

    # Insert into ChromaDB
    chroma.add(
        ids=[str(b['id']) for b in batch],
        embeddings=[b['embedding'] for b in batch],
        documents=[b['text'] for b in batch],
        metadatas=[{'session_id': b['session_id']} for b in batch]
    )
```

**Step 4: Verification**

```python
async def verify_migration():
    # Count records
    sqlite_count = await sqlite.execute('SELECT COUNT(*) FROM captures')
    pg_count = await postgres.fetchval('SELECT COUNT(*) FROM captures')
    chroma_count = collection.count()

    assert sqlite_count == pg_count == chroma_count, "Record count mismatch!"

    # Spot check random records
    sample_ids = random.sample(range(1, sqlite_count), 10)
    for id in sample_ids:
        sqlite_row = await sqlite.fetchone(f'SELECT * FROM captures WHERE id = {id}')
        pg_row = await postgres.fetchrow('SELECT * FROM captures WHERE id = $1', id)
        chroma_doc = collection.get(ids=[str(id)])

        assert sqlite_row['user_text'] == pg_row['user_text'], f"Mismatch in record {id}"
        assert chroma_doc is not None, f"Missing ChromaDB record {id}"

    print("âœ… Migration verified successfully!")
```

### Backward Compatibility

**During Migration:**
- v1 API continues using SQLite
- v2 API uses PostgreSQL + ChromaDB
- No breaking changes to v1 endpoints

**After Migration:**
- Keep SQLite as read-only archive
- All writes go to PostgreSQL
- Eventual decommissioning of SQLite

---

## Rollback Procedures

### Phase-Specific Rollback

**Phase 1-2: Pre-Data Migration**

```bash
# Simple feature flag toggle
curl -X POST /admin/feature-flags \
  -d '{"enable_v2_agents": false, "v2_traffic_percentage": 0}'

# Restart service
railway restart
```

**Phase 3+: Post-Data Migration**

```bash
# 1. Stop v2 traffic
curl -X POST /admin/feature-flags \
  -d '{"v2_traffic_percentage": 0}'

# 2. Restore SQLite backup
cp learning_captures_backup_20251121.db learning_captures.db

# 3. Restart with v1 only
railway restart --service app-v1

# 4. Verify v1 operation
curl https://app.railway.app/health
```

### Emergency Rollback Procedure

**Trigger Criteria:**
- Error rate > 5%
- Response time > 5 seconds
- Data corruption detected
- Critical bug in production

**Steps:**

1. **Immediate Response** (< 2 minutes)
   ```bash
   # Switch all traffic to v1
   railway env set ENABLE_V2=false
   railway env set V2_TRAFFIC_PERCENT=0
   railway restart
   ```

2. **Verify Recovery** (< 5 minutes)
   ```bash
   # Check health
   curl https://app.railway.app/health

   # Test conversation
   curl -X POST https://app.railway.app/api/conversation \
     -d '{"text": "test"}'
   ```

3. **Incident Response** (< 30 minutes)
   - Alert team via PagerDuty
   - Create incident ticket
   - Preserve logs and metrics
   - Begin root cause analysis

4. **Data Integrity Check** (< 1 hour)
   ```bash
   # Verify database consistency
   python scripts/verify_data_integrity.py

   # Check for corrupted records
   psql -c "SELECT COUNT(*) FROM captures WHERE user_text IS NULL"
   ```

5. **Communication** (< 2 hours)
   - Post-mortem document
   - User communication (if needed)
   - Timeline for retry

---

## Risk Mitigation

### Risk Matrix

| Risk | Probability | Impact | Severity | Mitigation |
|------|------------|--------|----------|------------|
| Data loss during migration | Medium | Critical | HIGH | Comprehensive backups, dry runs |
| Service downtime | Low | High | MEDIUM | Parallel operation, gradual cutover |
| Breaking API changes | Medium | High | MEDIUM | API versioning, backward compatibility |
| Cost overruns | High | Medium | MEDIUM | Cost monitoring, budget alerts |
| Performance degradation | Medium | High | MEDIUM | Load testing, performance benchmarks |
| Security vulnerabilities | Low | Critical | MEDIUM | Security audits, penetration testing |
| User adoption failure | Medium | Medium | MEDIUM | Gradual rollout, user feedback |

### Mitigation Strategies

**1. Data Loss Prevention**
- Automated daily backups to R2
- Point-in-time recovery with Litestream
- Transaction-based migration
- Dry runs in staging environment
- Verification scripts after each phase

**2. Service Continuity**
- Blue-green deployment strategy
- Feature flags for instant rollback
- Health checks and auto-recovery
- Multi-region deployment (future)

**3. API Compatibility**
- Semantic versioning (/v1, /v2)
- Deprecation warnings (6-month notice)
- Migration guides for API users
- Automated compatibility tests

**4. Cost Control**
- Budget alerts at 80% threshold
- Resource usage monitoring
- Auto-scaling limits configured
- Regular cost optimization reviews

**5. Performance Assurance**
- Load testing before each phase
- Performance regression tests
- SLA monitoring (P99 < 1.5s)
- Capacity planning

**6. Security Hardening**
- OAuth 2.0 / JWT authentication
- API rate limiting
- Input validation and sanitization
- Regular security audits
- Dependency vulnerability scanning

---

## Testing Strategy

### Test Pyramid

```
      /\
     /  \      E2E Tests (5%)
    /â”€â”€â”€â”€\     - Full user journeys
   /      \    - Cross-system integration
  /â”€â”€â”€â”€â”€â”€â”€â”€\   Integration Tests (20%)
 /          \  - API endpoints
/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ - Component interactions
              Unit Tests (75%)
              - Individual functions
              - Business logic
```

### Test Phases

**Phase 1: Foundation Testing**

```python
# Unit tests (pytest)
def test_conversation_handler():
    response = await conversation_handler.generate_response(
        "Hello", context=[]
    )
    assert response
    assert len(response) < 500

def test_audio_pipeline_transcription():
    audio = load_test_audio('hello.wav')
    text = await audio_pipeline.transcribe_audio(audio)
    assert 'hello' in text.lower()

# Integration tests
async def test_conversation_endpoint():
    response = await client.post('/api/conversation', json={
        'text': 'Hello, I am learning about AI'
    })
    assert response.status_code == 200
    assert 'agent_text' in response.json()

# Load tests (locust)
class ConversationUser(HttpUser):
    @task
    def send_message(self):
        self.client.post('/api/conversation', json={
            'text': 'Test message'
        })
```

**Phase 2-3: Migration Testing**

```python
# Data migration validation
async def test_data_migration():
    # Before migration
    sqlite_count = await get_sqlite_count()

    # Run migration
    await migrate_to_postgres()

    # After migration
    pg_count = await get_postgres_count()

    assert sqlite_count == pg_count

    # Verify sample records
    for id in sample_ids:
        sqlite_data = await fetch_from_sqlite(id)
        pg_data = await fetch_from_postgres(id)
        assert sqlite_data == pg_data

# Vector search accuracy
async def test_semantic_search():
    # Index known documents
    await index_documents([
        "I'm learning about neural networks",
        "Studying machine learning algorithms",
        "Reading about quantum computing"
    ])

    # Search with similar query
    results = await vector_search("artificial intelligence")

    # Should return neural networks and ML, not quantum
    assert results[0].text.contains("neural networks")
    assert results[1].text.contains("machine learning")
```

**Phase 4+: System Testing**

```python
# End-to-end user journey
async def test_full_conversation_flow():
    # 1. User sends voice message
    audio = record_audio("Hello, I'm learning Python")
    response1 = await send_audio(audio)
    assert response1['user_text'] == "Hello, I'm learning Python"

    # 2. Follow-up message
    audio2 = record_audio("I'm struggling with decorators")
    response2 = await send_audio(audio2)

    # Should reference previous context
    assert 'python' in response2['agent_text'].lower()

    # 3. Search for past conversations
    search_results = await search("python decorators")
    assert len(search_results) > 0

# Multi-agent coordination
async def test_agent_collaboration():
    # Send complex query requiring multiple agents
    response = await client.post('/v2/conversation', json={
        'text': 'Explain transformers in deep learning',
        'enable_research': True
    })

    # Should trigger ResearchAgent + ConversationAgent
    metadata = response.json()['metadata']
    assert 'research_agent' in metadata['agents_used']
    assert 'conversation_agent' in metadata['agents_used']
```

### Performance Testing

**Load Test Scenarios:**

```python
# Scenario 1: Normal load
# 10 concurrent users, 100 requests/sec
locust -f load_tests.py --users 10 --spawn-rate 2

# Scenario 2: Peak load
# 50 concurrent users, 500 requests/sec
locust -f load_tests.py --users 50 --spawn-rate 10

# Scenario 3: Stress test
# 200 concurrent users, find breaking point
locust -f load_tests.py --users 200 --spawn-rate 20
```

**Acceptance Criteria:**
- P50 latency < 1000ms
- P99 latency < 2000ms
- Error rate < 0.1%
- Throughput > 100 req/sec

### Regression Testing

**Automated Regression Suite:**
- Runs on every commit
- Covers all critical paths
- Includes performance benchmarks
- Alerts on degradation

```yaml
# .github/workflows/regression.yml
name: Regression Tests
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run unit tests
        run: pytest tests/unit
      - name: Run integration tests
        run: pytest tests/integration
      - name: Run performance tests
        run: python scripts/perf_test.py
      - name: Compare benchmarks
        run: python scripts/compare_benchmarks.py
```

---

## Deployment Strategy

### Deployment Environments

**1. Development (Local)**
- Individual developer machines
- Docker Compose for dependencies
- Hot reload enabled
- Mock external APIs

**2. Staging (Railway)**
- Mirrors production configuration
- Uses production-like data (anonymized)
- Testing ground for migrations
- Accessible to team only

**3. Production (Railway)**
- Auto-scaling enabled
- Multi-region (future)
- Continuous monitoring
- Blue-green deployments

### Deployment Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Commit    â”‚
â”‚   to main   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CI/CD     â”‚
â”‚  (GitHub    â”‚
â”‚   Actions)  â”‚
â”‚             â”‚
â”‚  â€¢ Lint     â”‚
â”‚  â€¢ Test     â”‚
â”‚  â€¢ Build    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Staging   â”‚
â”‚  Deployment â”‚
â”‚             â”‚
â”‚  â€¢ Auto     â”‚
â”‚  â€¢ Smoke    â”‚
â”‚    tests    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Manual approval
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Production  â”‚
â”‚  (Blue)     â”‚
â”‚             â”‚
â”‚  â€¢ Deploy   â”‚
â”‚  â€¢ Health   â”‚
â”‚    check    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Traffic switch
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Production  â”‚
â”‚  (Green)    â”‚
â”‚             â”‚
â”‚  â€¢ 100%     â”‚
â”‚    traffic  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Blue-Green Deployment

**Process:**

1. **Deploy Green** (new version)
   ```bash
   # Deploy v2 alongside v1
   railway up --service app-v2

   # Wait for health check
   railway logs --service app-v2
   ```

2. **Run Smoke Tests**
   ```bash
   # Test critical endpoints
   python scripts/smoke_test.py --env production-green
   ```

3. **Gradual Traffic Shift**
   ```bash
   # 5% to green
   railway env set TRAFFIC_SPLIT="blue:95,green:5"

   # Monitor for 1 hour
   # ...

   # 25% to green
   railway env set TRAFFIC_SPLIT="blue:75,green:25"

   # Continue until 100%
   ```

4. **Decommission Blue**
   ```bash
   # After 24 hours of stable green
   railway down --service app-v1
   ```

### Rollback Strategy

**Instant Rollback:**
```bash
# Switch traffic back to blue
railway env set TRAFFIC_SPLIT="blue:100,green:0"

# Or destroy green
railway down --service app-v2
```

**Data Rollback:**
```bash
# Restore from backup (if needed)
pg_restore --clean --dbname $DATABASE_URL latest_backup.dump

# Restore Redis from RDB
redis-cli --rdb redis_backup.rdb
```

---

## Timeline and Milestones

### Detailed Timeline (20 weeks)

**Weeks 1-2: Foundation**
- Week 1: Bug fixes, logging, error handling
- Week 2: Test suite, CI/CD, monitoring

**Weeks 3-4: Multi-Agent Core**
- Week 3: LangGraph setup, ConversationAgent
- Week 4: AnalysisAgent, ResearchAgent, Flow Nexus

**Weeks 5-6: Vector Memory**
- Week 5: ChromaDB, embedding pipeline
- Week 6: Neo4j, knowledge graph, data migration

**Weeks 7-8: Multi-Modal**
- Week 7: GPT-4V integration, document processing
- Week 8: VisionAgent, unified input handler

**Weeks 9-10: Real-Time Learning**
- Week 9: Feedback collection, training pipeline
- Week 10: Model versioning, A/B testing

**Weeks 11-12: Analytics Engine**
- Week 11: Pattern detection, topic modeling
- Week 12: Insights dashboard, recommendations

**Weeks 13-14: Modern Frontend**
- Week 13: Next.js setup, UI components
- Week 14: Real-time updates, PWA features

**Weeks 15-16: Mobile Apps**
- Week 15: React Native, voice/camera
- Week 16: Offline-first, push notifications

**Weeks 17-18: Cross-Device Sync**
- Week 17: CRDT protocol, conflict resolution
- Week 18: Encryption, device management

**Weeks 19-20: Production Deploy**
- Week 19: Load testing, performance optimization
- Week 20: Full cutover, v1 decommission

### Key Milestones

| Week | Milestone | Success Criteria |
|------|-----------|------------------|
| 2 | v1.0 Stabilized | 80% test coverage, < 0.1% errors |
| 4 | v2.0 Alpha | Multi-agent system working |
| 6 | v2.0 Beta | Vector search operational |
| 8 | Multi-Modal Ready | Vision + docs processing |
| 12 | Analytics Live | Insights dashboard complete |
| 14 | Frontend Rebuilt | Next.js app deployed |
| 16 | Mobile Apps | iOS + Android in beta |
| 18 | Full Feature Parity | All v2 features complete |
| 20 | Production Cutover | 100% traffic on v2.0 |

---

## Success Criteria

### Technical KPIs

**Performance:**
- âœ… P99 latency < 1.5 seconds
- âœ… Throughput > 100 requests/sec
- âœ… Error rate < 0.1%
- âœ… 99.9% uptime

**Quality:**
- âœ… Test coverage > 80%
- âœ… Zero critical bugs in production
- âœ… Security audit passed
- âœ… Accessibility WCAG AA compliant

**Scalability:**
- âœ… Auto-scaling operational
- âœ… Handles 1000+ concurrent users
- âœ… Database < 100ms query time
- âœ… Vector search < 200ms

### Business Metrics

**Adoption:**
- âœ… 95% of users migrated to v2
- âœ… < 5% rollback requests
- âœ… User satisfaction score > 4.5/5
- âœ… Mobile app downloads > 1000

**Engagement:**
- âœ… Average session time > 15 minutes
- âœ… 60% weekly active users
- âœ… 5+ insights generated per week
- âœ… 70% multi-device usage

**Cost Efficiency:**
- âœ… Total monthly cost < $100
- âœ… Cost per conversation < $0.01
- âœ… Infrastructure auto-scaling working
- âœ… No wasted resources

### Migration Success

**Data Integrity:**
- âœ… 100% of v1 data migrated
- âœ… Zero data loss during migration
- âœ… Checksums match on all records
- âœ… Search accuracy maintained

**Feature Parity:**
- âœ… All v1 features available in v2
- âœ… New features working as expected
- âœ… APIs backward compatible
- âœ… Documentation up to date

**Operational:**
- âœ… Monitoring dashboards complete
- âœ… Alerting configured
- âœ… Runbooks documented
- âœ… Team trained on v2 operations

---

## Appendix

### Useful Scripts

**Data Migration:**
```bash
# Backup before migration
./scripts/backup_all.sh

# Run migration
python scripts/migrate_to_v2.py --dry-run
python scripts/migrate_to_v2.py --execute

# Verify migration
python scripts/verify_migration.py
```

**Deployment:**
```bash
# Deploy to staging
railway up --environment staging

# Deploy to production (blue-green)
./scripts/blue_green_deploy.sh

# Rollback
./scripts/rollback_deployment.sh
```

**Monitoring:**
```bash
# Check system health
./scripts/health_check.sh

# View metrics
railway metrics

# Export logs
railway logs --since 1h > logs.txt
```

### Contact Information

**Migration Team:**
- Tech Lead: [Name]
- Database Admin: [Name]
- DevOps: [Name]
- QA Lead: [Name]

**Escalation:**
- On-call: [PagerDuty link]
- Slack: #migration-v2
- Email: team@example.com

---

**Document Version:** 1.0
**Last Updated:** 2025-11-21
**Next Review:** After Phase 1 completion
